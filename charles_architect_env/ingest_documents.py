import os
import shutil
import glob
import datetime
import ssl
import sys  # âœ… ×—×“×© â€“ ×“×¨×•×© ×œ×¢×¦×™×¨×ª ×”×¨×™×¦×”

from dotenv import load_dotenv

# âœ… ×˜×¢×™× ×ª ××©×ª× ×™ ×¡×‘×™×‘×”
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    print("âŒ ×©×’×™××”: ××¤×ª×— OPENAI_API_KEY ×œ× ×”×•×’×“×¨ ×‘×§×•×‘×¥ .env ××• ×‘××©×ª× ×™ ×”×¡×‘×™×‘×”")
    sys.exit(1)
else:
    print("ğŸ”‘ ××¤×ª×— × ×˜×¢×Ÿ ×‘×”×¦×œ×—×”")

# âœ… ×¢×§×™×¤×ª ×©×’×™××ª ×ª×¢×•×“×ª SSL ××•×œ OpenAI
ssl._create_default_https_context = ssl._create_unverified_context

from langchain_community.document_loaders import (
    PyMuPDFLoader,
    TextLoader,
    UnstructuredExcelLoader,
    UnstructuredFileLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
CHROMA_DIR = os.getenv("VECTORSTORE_DIR", os.path.join(BASE_DIR, "vectorstore"))
LOG_FILE = os.path.join(BASE_DIR, "ingest_log.txt")

SUPPORTED_EXTENSIONS = [
    ".txt", ".md", ".py", ".ipynb", ".pdf", ".xlsx", ".xls", ".csv", ".json", ".log", ".html"
]
MAX_DOCS_PER_BATCH = 100

def write_log(successful, unsupported, errors):
    with open(LOG_FILE, "w", encoding="utf-8") as f:
        f.write("âœ… ×§×‘×¦×™× ×©× ×§×œ×˜×• ×‘×”×¦×œ×—×”:\n")
        for path in successful:
            f.write(f"{path}\n")
        f.write("\nâš ï¸ ×§×‘×¦×™× ×©×œ× × ×ª××›×™×:\n")
        for path in unsupported:
            f.write(f"{path}\n")
        f.write("\nâŒ ×§×‘×¦×™× ×©×’×¨××• ×œ×©×’×™××”:\n")
        for path, msg in errors:
            f.write(f"{path} | {msg}\n")

def clean_vectorstore():
    if os.path.exists(CHROMA_DIR):
        print("ğŸ§¹ ××•×—×§ Vector DB ×™×©×Ÿ...")
        shutil.rmtree(CHROMA_DIR)

def get_file_metadata(path):
    filename = os.path.basename(path)
    file_type = os.path.splitext(filename)[1][1:].lower()
    created_at = datetime.datetime.fromtimestamp(os.path.getctime(path)).strftime("%Y-%m-%d %H:%M:%S")

    if file_type in ["py", "ipynb"]:
        category = "code"
    elif file_type in ["pdf"]:
        category = "spec"
    elif file_type in ["xlsx", "xls", "csv"]:
        category = "excel"
    elif file_type in ["log"]:
        category = "log"
    else:
        category = "other"

    return {
        "source": os.path.relpath(path, BASE_DIR),
        "filename": filename,
        "file_type": file_type,
        "created_at": created_at,
        "category": category
    }

def load_documents():
    docs, successful, unsupported, errors = [], [], [], []
    all_files = glob.glob(f"{BASE_DIR}/**/*", recursive=True)
    print(f"ğŸ“‚ × ××¦××• {len(all_files)} ×§×‘×¦×™×. ×˜×•×¢×Ÿ × ×ª×•× ×™×...")
    loaded_paths = set()

    for path in all_files:
        ext = os.path.splitext(path)[1].lower()
        if path in loaded_paths:
            continue
        try:
            loaded = []
            if ext in [".txt", ".md", ".py", ".ipynb"]:
                loaded = TextLoader(path, encoding="utf-8").load()
            elif ext == ".pdf":
                loaded = PyMuPDFLoader(path).load()
            elif ext in [".xlsx", ".xls"]:
                loaded = UnstructuredExcelLoader(path).load()
            elif ext in [".csv", ".json", ".log", ".html"]:
                loaded = UnstructuredFileLoader(path).load()
            else:
                print(f"âš ï¸ ×¤×•×¨××˜ ×œ× × ×ª××š: {path}")
                unsupported.append(path)
                continue

            metadata = get_file_metadata(path)
            for doc in loaded:
                doc.metadata.update(metadata)

            docs += loaded
            successful.append(path)
            loaded_paths.add(path)
            print(f"âœ… × ×˜×¢×Ÿ: {path}")

        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×§×¨×™××”: {path} | {e}")
            errors.append((path, str(e)))

    write_log(successful, unsupported, errors)
    return docs

def batch_documents(docs, batch_size):
    for i in range(0, len(docs), batch_size):
        yield docs[i:i + batch_size]

def ingest():
    clean_vectorstore()
    print("ğŸ“¥ ×˜×•×¢×Ÿ ××¡××›×™×...")
    raw_docs = load_documents()

    print("ğŸ”— ××¤×¦×œ ×œ××¡××›×™× ×§×˜× ×™×...")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs = splitter.split_documents(raw_docs)
    print(f"ğŸ“‘ ×œ××—×¨ ×¤×™×¦×•×œ: {len(docs)} ××¡××›×™×")

    print("ğŸ§  ××—×©×‘ Embeddings ×‘××¦×•×•×ª...")
    embeddings = OpenAIEmbeddings()
    vectordb = Chroma(persist_directory=CHROMA_DIR, embedding_function=embeddings)

    for i, doc_batch in enumerate(batch_documents(docs, MAX_DOCS_PER_BATCH)):
        try:
            print(f"ğŸ”¢ ×§×‘×•×¦×” {i+1}: {len(doc_batch)} ××¡××›×™×")
            vectordb.add_documents(doc_batch)
        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×©×œ×‘ ×”×”×˜××¢×” (Embeddings): {e}")
            print("ğŸ” ×”××©×š ×¢× ×”×§×‘×•×¦×” ×”×‘××”...")

    print("âœ… ×”×•×©×œ×! Vector DB × ×‘× ×” ×•× ×©××¨. ×œ×•×’ × ×›×ª×‘ ×œ-ingest_log.txt")

if __name__ == "__main__":
    ingest()
